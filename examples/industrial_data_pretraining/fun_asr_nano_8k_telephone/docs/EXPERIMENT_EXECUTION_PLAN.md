# ä¸‰é˜¶æ®µå¾®è°ƒå®éªŒæ‰§è¡Œè®¡åˆ’

> **å®éªŒå¯åŠ¨æ—¶é—´**: 2026-01-05  
> **æ‰§è¡Œäºº**: Algorithm Research Team  
> **ç›®æ ‡**: å°† Fun-ASR-Nano é€‚é…åˆ° 8kHz ä¸­æ–‡ç”µè¯è¯­éŸ³è¯†åˆ«åœºæ™¯

---

## ä¸€ã€å®éªŒç¯å¢ƒé…ç½® âœ…

### ç¡¬ä»¶èµ„æº
- **GPU**: 8Ã— NVIDIA H100 80GB (ä½¿ç”¨ GPU 2,3)
- **GPU å†…å­˜**: 81GB Ã— 2 = 162GB å¯ç”¨
- **è®¡ç®—å¡**: GPU 2, 3 (é¿å…ä¸å…¶ä»–ä»»åŠ¡å†²çª)

### è½¯ä»¶ç¯å¢ƒ
- **FunASR**: v1.2.9 âœ…
- **Python**: 3.x âœ…
- **PyTorch**: å·²å®‰è£… âœ…
- **ä¾èµ–**: å·²å®‰è£… âœ…

### å·¥ä½œç›®å½•
```
/workspace/share/LLMFunASR/examples/industrial_data_pretraining/fun_asr_nano_8k_telephone/
â”œâ”€â”€ run_experiment.sh          # ä¸»å®éªŒè„šæœ¬ âœ…
â”œâ”€â”€ test_data_prep.sh          # æ•°æ®å‡†å¤‡æµ‹è¯• âœ…
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ stage1_encoder_adapt.yaml    # Stage 1 é…ç½® âœ…
â”‚   â”œâ”€â”€ stage2_adapter_align.yaml    # Stage 2 é…ç½® âœ…
â”‚   â””â”€â”€ stage3_lora_domain.yaml      # Stage 3 é…ç½® âœ…
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data_simulation.py           # ç”µè¯ä¿¡é“æ¨¡æ‹Ÿ âœ…
â”‚   â””â”€â”€ prepare_training_data.py     # æ•°æ®æ ¼å¼è½¬æ¢ âœ… (å·²æ·»åŠ  filter_annotation_error)
â””â”€â”€ exp_output/                      # è¾“å‡ºç›®å½•
```

---

## äºŒã€æ•°æ®å‡†å¤‡ âœ…

### Stage 1-2 è®­ç»ƒæ•°æ®: WenetSpeech (1000h)
- **è·¯å¾„**: `/data/speech/open/data/openslr/chinese/WenetSpeech/jsonl/funasr_jsonl/`
- **è®­ç»ƒé›†**: `train_M.jsonl` (65,170 samples)
- **éªŒè¯é›†**: `eval_dev.jsonl` (13,825 samples)
- **æ ¼å¼**: å·²åˆ‡åˆ†çš„ 16kHz WAV + JSONL

### Stage 3 è®­ç»ƒæ•°æ®: ç”µè¯å¤–å‘¼çœŸå®æ•°æ® (50h)
- **è·¯å¾„**: `/data/speech/labeled/yx_telecall/sale/training_data/yx_telecall_v2_1_2025-12-22/manifests/`
- **è®­ç»ƒé›†**: `audio_neutral.jsonl` (12,172 samples)
- **éªŒè¯é›†**: `audio_quiet.jsonl` (9,001 samples)
- **æ ¼å¼**: 8kHz ç”µè¯å½•éŸ³ + JSONL
- **è´¨é‡**: åŒ…å« `<ANNOTATION_ERROR>` æ ‡è®°ï¼Œéœ€è¿‡æ»¤

### æ•°æ®å¢å¼ºç­–ç•¥
WenetSpeech â†’ ç”µè¯ä¿¡é“æ¨¡æ‹Ÿ:
1. é™é‡‡æ ·åˆ° 8kHz
2. G.711 Î¼-law ç¼–è§£ç 
3. 300-3400Hz å¸¦é€šæ»¤æ³¢
4. VoIP ä¸¢å¸§æ¨¡æ‹Ÿ (1-2%)
5. Babble Noise (10-25dB SNR)
6. ä¸Šé‡‡æ ·å› 16kHz (ä¿ç•™å¤±çœŸ)

---

## ä¸‰ã€å®éªŒæ‰§è¡Œæ­¥éª¤

### ğŸ”§ Step 0: æ•°æ®å‡†å¤‡æµ‹è¯• (é¢„è®¡ 10 åˆ†é’Ÿ)

```bash
cd /workspace/share/LLMFunASR/examples/industrial_data_pretraining/fun_asr_nano_8k_telephone

# è¿è¡Œå°è§„æ¨¡æµ‹è¯• (10 samples)
bash test_data_prep.sh
```

**éªŒè¯ç‚¹**:
- âœ“ WenetSpeech æ•°æ®å¯è¯»
- âœ“ ç”µè¯æ•°æ®å¯è¯»
- âœ“ æ¨¡æ‹Ÿè„šæœ¬æ­£å¸¸è¿è¡Œ
- âœ“ æ ¼å¼è½¬æ¢æ­£å¸¸
- âœ“ æ•°æ®éªŒè¯é€šè¿‡

---

### ğŸ“Š Stage 0: å®Œæ•´æ•°æ®å‡†å¤‡ (é¢„è®¡ 4-6 å°æ—¶)

```bash
# æ¨¡æ‹Ÿ WenetSpeech â†’ 8kHz ç”µè¯éŸ³é¢‘ (65k samples)
bash run_experiment.sh 0 0
```

**è¾“å‡º**:
- `exp_output/data/simulated_8k_telephone/train_formatted.jsonl` (~65k samples)
- `exp_output/data/simulated_8k_telephone/dev_formatted.jsonl` (~14k samples)
- `exp_output/data/real_8k_telephone/train_formatted.jsonl` (~10k samples, è¿‡æ»¤å)
- `exp_output/data/real_8k_telephone/dev_formatted.jsonl` (~9k samples, è¿‡æ»¤å)

**æ£€æŸ¥ç‚¹**:
```bash
# éªŒè¯æ•°æ®é‡
wc -l exp_output/data/simulated_8k_telephone/*.jsonl
wc -l exp_output/data/real_8k_telephone/*.jsonl

# æ£€æŸ¥æ ·æœ¬
head -1 exp_output/data/simulated_8k_telephone/train_formatted.jsonl | python -m json.tool
```

---

### ğŸ¯ Stage 1: Audio Encoder é€‚é… (é¢„è®¡ 2-3 å¤©)

**ç›®æ ‡**: è®© Encoder é€‚åº” 8kHz é¢‘è°±ç‰¹å¾

```bash
bash run_experiment.sh 1 1
```

**è®­ç»ƒé…ç½®**:
- **å¯è®­ç»ƒå‚æ•°**: Audio Encoder (~150M)
- **å†»ç»“æ¨¡å—**: Adaptor, CTC, LLM
- **å­¦ä¹ ç‡**: 1e-4
- **Batch size**: 8192 tokens
- **Epoch**: 10
- **GPU**: 2 Ã— H100

**ç›‘æ§æŒ‡æ ‡**:
```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f exp_output/exp/8k_telephone/stage1_encoder_*/train.log

# å…³é”®æŒ‡æ ‡
# - Loss ä¸‹é™ 30%+
# - CER åœ¨éªŒè¯é›†ä¸Šæ”¹å–„
```

**é¢„æœŸæ•ˆæœ**:
- âœ“ Loss ä» ~8.0 é™è‡³ ~5.5
- âœ“ Encoder å­¦åˆ° 8kHz é¢‘è°±ç‰¹å¾
- âœ“ Checkpoint ä¿å­˜åˆ° `exp_output/exp/8k_telephone/stage1_checkpoint.txt`

---

### ğŸ”„ Stage 2: Adapter & CTC å¯¹é½ (é¢„è®¡ 2-3 å¤©)

**ç›®æ ‡**: é‡æ–°å¯¹é½ 8kHz ç‰¹å¾åˆ° Token ç©ºé—´

```bash
bash run_experiment.sh 2 2
```

**è®­ç»ƒé…ç½®**:
- **å¯è®­ç»ƒå‚æ•°**: Adaptor (~4M) + CTC (~8M)
- **å†»ç»“æ¨¡å—**: Encoder (Stage 1), LLM
- **å­¦ä¹ ç‡**: 5e-5
- **Batch size**: 8192 tokens
- **Epoch**: 10

**ç›‘æ§æŒ‡æ ‡**:
```bash
tail -f exp_output/exp/8k_telephone/stage2_adapter_*/train.log

# å…³é”®æŒ‡æ ‡
# - CTC Loss æ˜¾è‘—ä¸‹é™
# - CER è¿›ä¸€æ­¥æ”¹å–„
```

**é¢„æœŸæ•ˆæœ**:
- âœ“ CTC Loss ä¸‹é™ 40%+
- âœ“ CER < 10% (éªŒè¯é›†)
- âœ“ Checkpoint ä¿å­˜åˆ° `exp_output/exp/8k_telephone/stage2_checkpoint.txt`

---

### ğŸš€ Stage 3: LLM LoRA ä¸šåŠ¡é€‚é… (é¢„è®¡ 1-2 å¤©)

**ç›®æ ‡**: æ³¨å…¥ç”µè¯å¤–å‘¼ä¸šåŠ¡æœ¯è¯­

```bash
bash run_experiment.sh 3 3
```

**è®­ç»ƒé…ç½®**:
- **å¯è®­ç»ƒå‚æ•°**: LLM LoRA (r=16, ~2M)
- **å†»ç»“æ¨¡å—**: Encoder, Adaptor, CTC
- **å­¦ä¹ ç‡**: 1e-5
- **Batch size**: 4096 tokens
- **Epoch**: 20
- **æ•°æ®**: çœŸå®ç”µè¯å¤–å‘¼æ•°æ® (~10k samples)

**ç›‘æ§æŒ‡æ ‡**:
```bash
tail -f exp_output/exp/8k_telephone/stage3_lora_*/train.log

# å…³é”®æŒ‡æ ‡
# - åœ¨ä¸šåŠ¡æœ¯è¯­ä¸Šçš„å‡†ç¡®ç‡æå‡
# - KWER (å…³é”®è¯é”™è¯¯ç‡) æ”¹å–„
```

**é¢„æœŸæ•ˆæœ**:
- âœ“ ä¸šåŠ¡å…³é”®è¯å‡†ç¡®ç‡ > 95%
- âœ“ æ•´ä½“ CER < 8%
- âœ“ æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ° `exp_output/exp/8k_telephone/final_checkpoint.txt`

---

## å››ã€å®éªŒè¯„ä¼°

### è¯„ä¼°è„šæœ¬

```bash
# 1. ç”Ÿæˆè¯†åˆ«ç»“æœ
python inference_8k.py \
    --model_path $(cat exp_output/exp/8k_telephone/final_checkpoint.txt) \
    --test_data /path/to/test.jsonl \
    --output_dir ./evaluation_results

# 2. è®¡ç®— KWER
python evaluate_keywords.py \
    --results ./evaluation_results/results.jsonl \
    --keywords keywords_example.txt \
    --output ./evaluation_results/kwer_metrics.json
```

### è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | Baseline (16kHz) | Stage 1 | Stage 2 | Stage 3 (ç›®æ ‡) |
|------|------------------|---------|---------|----------------|
| CER  | ~12% (8kHz)      | ~10%    | ~8%     | **< 8%**       |
| WER  | ~18%             | ~15%    | ~12%    | **< 12%**      |
| KWER (ä¸šåŠ¡è¯) | ~15%    | ~12%    | ~10%    | **< 5%**       |

---

## äº”ã€æ—¶é—´è§„åˆ’

| é˜¶æ®µ | ä»»åŠ¡ | é¢„è®¡æ—¶é—´ | ç´¯è®¡æ—¶é—´ |
|------|------|----------|----------|
| Step 0 | æ•°æ®å‡†å¤‡æµ‹è¯• | 10 åˆ†é’Ÿ | 10 åˆ†é’Ÿ |
| Stage 0 | å®Œæ•´æ•°æ®å‡†å¤‡ | 4-6 å°æ—¶ | 6 å°æ—¶ |
| Stage 1 | Encoder é€‚é… | 2-3 å¤© | 3.25 å¤© |
| Stage 2 | Adapter å¯¹é½ | 2-3 å¤© | 6.25 å¤© |
| Stage 3 | LoRA ä¸šåŠ¡é€‚é… | 1-2 å¤© | 7.5 å¤© |
| è¯„ä¼° | ç»“æœåˆ†æ | 0.5 å¤© | **8 å¤©** |

**æ€»è®¡**: ~8 å¤© (åŒ…å«æ•°æ®å‡†å¤‡ã€è®­ç»ƒã€è¯„ä¼°)

---

## å…­ã€é£é™©ä¸åº”å¯¹

### é£é™©ç‚¹ 1: GPU OOM
- **åº”å¯¹**: å‡å° batch_size (8192 â†’ 4096)
- **é…ç½®**: ä¿®æ”¹ conf/*.yaml ä¸­çš„ `batch_size`

### é£é™©ç‚¹ 2: Stage 1 Loss ä¸æ”¶æ•›
- **åº”å¯¹**: 
  1. é™ä½å­¦ä¹ ç‡ (1e-4 â†’ 5e-5)
  2. å¢åŠ  warmup_steps (1000 â†’ 2000)
  3. æ£€æŸ¥æ•°æ®è´¨é‡

### é£é™©ç‚¹ 3: Stage 3 è¿‡æ‹Ÿåˆ (æ•°æ®å°‘)
- **åº”å¯¹**: 
  1. å¢å¤§ gradient accumulation (2 â†’ 4)
  2. é™ä½å­¦ä¹ ç‡ (1e-5 â†’ 5e-6)
  3. Early stopping

---

## ä¸ƒã€Checkpoint ç®¡ç†

### è‡ªåŠ¨ä¿å­˜
æ¯ä¸ª Stage å®Œæˆåï¼Œæœ€ä½³æ¨¡å‹è‡ªåŠ¨ä¿å­˜:
```
exp_output/exp/8k_telephone/
â”œâ”€â”€ stage1_checkpoint.txt  â†’ stage1_encoder_20260105_*/model.pt.avg
â”œâ”€â”€ stage2_checkpoint.txt  â†’ stage2_adapter_20260105_*/model.pt.avg
â””â”€â”€ final_checkpoint.txt   â†’ stage3_lora_20260105_*/model.pt.avg
```

### æ‰‹åŠ¨å¤‡ä»½
```bash
# å¤‡ä»½é‡è¦ checkpoint
cp -r exp_output/exp/8k_telephone/stage*_20260105_* /backup/fun_asr_nano_8k/
```

---

## å…«ã€å®éªŒå¯åŠ¨æ¸…å•

- [x] ç¯å¢ƒæ£€æŸ¥ (GPU, Python, FunASR)
- [x] æ•°æ®è·¯å¾„ç¡®è®¤ (WenetSpeech, ç”µè¯æ•°æ®)
- [x] è„šæœ¬å‡†å¤‡ (run_experiment.sh, test_data_prep.sh)
- [x] é…ç½®æ–‡ä»¶éªŒè¯ (stage*.yaml)
- [x] æ•°æ®å‡†å¤‡è„šæœ¬å¢å¼º (filter_annotation_error)
- [ ] **è¿è¡Œå°è§„æ¨¡æµ‹è¯•** (`bash test_data_prep.sh`)
- [ ] **å¯åŠ¨ Stage 0** (`bash run_experiment.sh 0 0`)
- [ ] **å¯åŠ¨ Stage 1** (`bash run_experiment.sh 1 1`)
- [ ] **å¯åŠ¨ Stage 2** (`bash run_experiment.sh 2 2`)
- [ ] **å¯åŠ¨ Stage 3** (`bash run_experiment.sh 3 3`)
- [ ] **è¿è¡Œè¯„ä¼°** (`python evaluate_keywords.py`)

---

## ä¹ã€ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³æ‰§è¡Œ

```bash
cd /workspace/share/LLMFunASR/examples/industrial_data_pretraining/fun_asr_nano_8k_telephone

# 1. è¿è¡Œæµ‹è¯•
bash test_data_prep.sh

# 2. å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œå¯åŠ¨å®Œæ•´å®éªŒ
bash run_experiment.sh 0 3  # ä¸€é”®è¿è¡Œæ‰€æœ‰é˜¶æ®µ
```

### ç›‘æ§æ–¹å¼

```bash
# å®æ—¶ç›‘æ§è®­ç»ƒæ—¥å¿—
watch -n 60 'tail -20 exp_output/exp/8k_telephone/*/train.log | grep -E "(loss|CER|epoch)"'

# æ£€æŸ¥ GPU ä½¿ç”¨
watch -n 5 'nvidia-smi'
```

---

**å®éªŒå‡†å¤‡å®Œæˆï¼å‡†å¤‡å¼€å§‹æ‰§è¡Œã€‚** ğŸš€
