# Configuration Alignment for Fun-ASR-Nano 8kHz Telephone Fine-tuning

## Overview
This document provides a detailed mapping of all configuration parameters between the **pretrained model** (`/gpfs01/nfs_share/finrc/liangguang/cache/modelscope/models/FunAudioLLM/Fun-ASR-Nano-2512/config.yaml`) and the **fine-tuning experiments** in this directory.

### Legend
- ğŸ”µ **ALIGNED**: Same as pretrained model (no modification)
- ğŸŸ¡ **MODIFIED**: Changed for fine-tuning (highlighted)
- ğŸŸ  **NEW**: Not in pretrained config (added for fine-tuning)
- âšª **OMITTED**: Not set in fine-tuning (uses code default)

---

## 1. Network Architecture

### 1.1 Model Definition
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `model` | `FunASRNano` | ğŸ”µ `FunASRNano` | ğŸ”µ `FunASRNano` | Same model type |
| `model_conf.lsm_weight` | `0.1` | ğŸ”µ `0.1` | ğŸ”µ `0.1` | Label smoothing |
| `model_conf.length_normalized_loss` | `true` | ğŸ”µ `true` | ğŸ”µ `true` | Loss normalization |

---

## 2. Audio Encoder Configuration

### 2.1 Audio Encoder Type & Basic Config
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `audio_encoder` | `SenseVoiceEncoderSmall` | ğŸ”µ Same | ğŸ”µ Same | Same encoder type |
| `audio_encoder_conf.output_size` | `512` | ğŸ”µ `512` | ğŸ”µ `512` | Output dimension |
| `audio_encoder_conf.attention_heads` | `4` | âšª OMITTED | âšª OMITTED | Uses default from pretrained |
| `audio_encoder_conf.linear_units` | `2048` | âšª OMITTED | âšª OMITTED | Uses default from pretrained |
| `audio_encoder_conf.num_blocks` | `50` | âšª OMITTED | âšª OMITTED | Uses default from pretrained |
| `audio_encoder_conf.tp_blocks` | `20` | âšª OMITTED | âšª OMITTED | Uses default from pretrained |

### 2.2 Audio Encoder Fine-tuning Strategy
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `audio_encoder_conf.freeze` | ğŸ”µ `true` | ğŸŸ¡ **`false`** | ğŸŸ¡ **`false`** | ğŸ”´ **CRITICAL**: Encoder trainable in fine-tuning |
| `audio_encoder_conf.freeze_layer_num` | ğŸ”µ `-1` | âšª OMITTED | âšª OMITTED | Not used by FunASRNano |
| `audio_encoder_conf.dropout_rate` | `0.1` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.positional_dropout_rate` | `0.1` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.attention_dropout_rate` | `0.1` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.input_layer` | `pe` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.pos_enc_class` | `SinusoidalPositionEncoder` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.normalize_before` | `true` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.kernel_size` | `11` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.sanm_shfit` | `0` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.selfattention_layer_type` | `sanm` | âšª OMITTED | âšª OMITTED | Uses default |
| `audio_encoder_conf.feat_permute` | `true` | âšª OMITTED | âšª OMITTED | Uses default |

---

## 3. LLM Configuration

### 3.1 LLM Model & Setup
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `llm` | `Qwen3-0.6b` | ğŸ”µ `Qwen3-0.6b` | ğŸ”µ `Qwen3-0.6b` | Same LLM model |
| `llm_conf.hub` | `hf` | ğŸ”µ `hf` | âšª OMITTED | HuggingFace hub (uses default) |
| `llm_conf.freeze` | `true` | ğŸ”µ `true` | ğŸ”µ `true` | LLM is frozen (not trainable) |
| `llm_conf.llm_dtype` | `bf16` | ğŸ”µ `bf16` | âšª OMITTED | BFloat16 precision (uses default) |
| `llm_conf.init_param_path` | `Qwen3-0.6B` | ğŸ”µ `Qwen3-0.6B` | âšª OMITTED | LLM weights path (uses default) |

### 3.2 LLM LoRA Configuration
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `llm_conf.use_lora` | `false` | âšª OMITTED | âšª OMITTED | No LoRA (uses default) |
| `llm_conf.lora_conf.*` | All defined | âšª OMITTED | âšª OMITTED | LoRA disabled |

---

## 4. Audio Adaptor Configuration

### 4.1 Adaptor Type & Architecture
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `audio_adaptor` | `Transformer` | ğŸ”µ `Transformer` | ğŸ”µ `Transformer` | Same adaptor type |
| `audio_adaptor_conf.downsample_rate` | `1` | ğŸ”µ `1` | âšª OMITTED | No downsampling |
| `audio_adaptor_conf.use_low_frame_rate` | `true` | ğŸ”µ `true` | âšª OMITTED | Uses default |
| `audio_adaptor_conf.ffn_dim` | `2048` | ğŸ”µ `2048` | âšª OMITTED | FFN dimension |
| `audio_adaptor_conf.llm_dim` | `1024` | ğŸ”µ `1024` | âšª OMITTED | LLM dimension |
| `audio_adaptor_conf.encoder_dim` | `512` | ğŸ”µ `512` | âšª OMITTED | Encoder dimension |
| `audio_adaptor_conf.n_layer` | `2` | ğŸ”µ `2` | âšª OMITTED | Number of layers |

### 4.2 Adaptor Fine-tuning Strategy
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `audio_adaptor_conf.freeze` | `true` | ğŸ”µ `true` | ğŸ”µ `true` | Adaptor is frozen (not trainable) |

---

## 5. CTC Decoder Configuration

### 5.1 Decoder Type & Architecture
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `ctc_decoder` | `Transformer` | ğŸ”µ `Transformer` | ğŸ”µ `Transformer` | Same decoder type |
| `ctc_decoder_conf.downsample_rate` | `1` | âšª OMITTED | âšª OMITTED | No downsampling |
| `ctc_decoder_conf.ffn_dim` | `2048` | âšª OMITTED | âšª OMITTED | Uses default |
| `ctc_decoder_conf.llm_dim` | `512` | âšª OMITTED | âšª OMITTED | Uses default |
| `ctc_decoder_conf.encoder_dim` | `512` | âšª OMITTED | âšª OMITTED | Uses default |
| `ctc_decoder_conf.n_layer` | `5` | âšª OMITTED | âšª OMITTED | Uses default |

### 5.2 CTC Decoder Fine-tuning Strategy
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `ctc_decoder_conf.freeze` | `false` | ğŸ”µ `false` | ğŸ”µ `true` | ğŸŸ¡ **DIFFERENT STAGES**: config_8k_telephone trains CTC; stage1 freezes it |
| `detach_ctc_decoder` | `true` | âšª OMITTED | âšª OMITTED | Uses default |

### 5.3 CTC Weight
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `ctc_weight` | `1.0` | ğŸ”µ `1.0` | âšª OMITTED | CTC loss weight |
| `ctc_conf.dropout_rate` | `0.0` | âšª OMITTED | âšª OMITTED | Uses default |
| `ctc_conf.ctc_type` | `builtin` | âšª OMITTED | âšª OMITTED | Uses default |
| `ctc_conf.reduce` | `true` | âšª OMITTED | âšª OMITTED | Uses default |
| `ctc_conf.ignore_nan_grad` | `true` | âšª OMITTED | âšª OMITTED | Uses default |

---

## 6. Frontend Configuration

### 6.1 Frontend Type & Sampling Rate
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `frontend` | `WavFrontend` | ğŸ”µ `WavFrontend` | ğŸ”µ `WavFrontend` | Same frontend type |
| `frontend_conf.fs` | `16000` | ğŸŸ¡ **`16000`** | ğŸŸ¡ **`8000`** | ğŸ”´ **CRITICAL**: Sample rate differs by stage |
| `frontend_conf.window` | `hamming` | ğŸ”µ `hamming` | ğŸ”µ `hamming` | Window type |
| `frontend_conf.n_mels` | `80` | ğŸ”µ `80` | ğŸ”µ `80` | Mel features |
| `frontend_conf.frame_length` | `25` | ğŸ”µ `25` | ğŸ”µ `25` | Frame length (ms) |
| `frontend_conf.frame_shift` | `10` | ğŸ”µ `10` | ğŸ”µ `10` | Frame shift (ms) |
| `frontend_conf.lfr_m` | `7` | ğŸ”µ `7` | ğŸ”µ `7` | LFR mode |
| `frontend_conf.lfr_n` | `6` | ğŸ”µ `6` | ğŸ”µ `6` | LFR factor |
| `frontend_conf.cmvn_file` | `null` | âšª OMITTED | âšª OMITTED | No CMVN normalization |

### 6.2 SpecAugmentation
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `specaug` | âšª NOT SET | ğŸŸ¡ **`SpecAugLFR`** | ğŸŸ¡ **`SpecAugLFR`** | ğŸŸ  **NEW**: SpecAug added for fine-tuning |
| `specaug_conf.apply_time_warp` | âšª N/A | ğŸŸ¡ `false` | ğŸŸ¡ `false` | No time warping |
| `specaug_conf.time_warp_window` | âšª N/A | âšª OMITTED | âšª OMITTED | Uses default |
| `specaug_conf.apply_freq_mask` | âšª N/A | ğŸŸ¡ `true` | ğŸŸ¡ `true` | Frequency masking enabled |
| `specaug_conf.freq_mask_width_range` | âšª N/A | ğŸŸ¡ `[0, 15]` | ğŸŸ¡ `[0, 10]` | ğŸŸ¡ **MODIFIED**: Smaller range for 8kHz |
| `specaug_conf.lfr_rate` | âšª N/A | ğŸŸ¡ `6` | ğŸŸ¡ `6` | LFR rate |
| `specaug_conf.num_freq_mask` | âšª N/A | ğŸŸ¡ `1` | ğŸŸ¡ `1` | Number of freq masks |
| `specaug_conf.apply_time_mask` | âšª N/A | ğŸŸ¡ `true` | ğŸŸ¡ `true` | Time masking enabled |
| `specaug_conf.time_mask_width_range` | âšª N/A | ğŸŸ¡ `[0, 10]` | ğŸŸ¡ `[0, 50]` | ğŸŸ¡ **MODIFIED**: Different ranges for different stages |
| `specaug_conf.num_time_mask` | âšª N/A | ğŸŸ¡ `1` | ğŸŸ¡ `2` | ğŸŸ¡ **MODIFIED**: More masking in stage1 |

---

## 7. Training Configuration

### 7.1 Basic Training Parameters
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `train_conf.accum_grad` | `1` | ğŸ”µ `1` | ğŸ”µ `1` | Gradient accumulation steps |
| `train_conf.grad_clip` | `5` | ğŸ”µ `5` | ğŸ”µ `5` | Gradient clipping value |
| `train_conf.max_epoch` | `2` | ğŸŸ¡ **`20`** | ğŸŸ¡ **`10`** | ğŸ”´ **MODIFIED**: More epochs for fine-tuning |
| `train_conf.keep_nbest_models` | `200` | ğŸŸ¡ **`10`** | ğŸŸ¡ **`5`** | ğŸŸ¡ **MODIFIED**: Fewer models kept |
| `train_conf.log_interval` | `100` | ğŸŸ¡ **`10`** | ğŸŸ¡ **`10`** | ğŸŸ¡ **MODIFIED**: More frequent logging |

### 7.2 Checkpoint & Validation Strategy
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `train_conf.validate_interval` | `2000` | âšª OMITTED | ğŸŸ¡ `1000` | ğŸ”´ **stage1**: More frequent validation |
| `train_conf.save_checkpoint_interval` | `2000` | âšª OMITTED | ğŸŸ¡ `1000` | ğŸ”´ **stage1**: More frequent checkpoints |
| `train_conf.avg_nbest_model` | `100` | âšª OMITTED | ğŸŸ¡ `3` | ğŸŸ¡ **stage1**: Model averaging |
| `train_conf.use_bf16` | `false` | âšª OMITTED | ğŸŸ¡ `false` | No mixed precision |
| `train_conf.use_deepspeed` | `true` | ğŸŸ¡ **`false`** | ğŸŸ¡ **`false`** | ğŸŸ¡ **MODIFIED**: DeepSpeed disabled for fine-tuning |
| `train_conf.deepspeed_config` | `null` | ğŸ”µ `null` | âšª OMITTED | No DeepSpeed config |
| `train_conf.save_init_model` | `false` | âšª OMITTED | âšª OMITTED | Uses default |
| `train_conf.effective_save_name_excludes` | `[llm.]` | âšª OMITTED | âšª OMITTED | Exclude LLM from checkpoints |
| `train_conf.resume` | `true` | âšª OMITTED | âšª OMITTED | Resume training |

---

## 8. Optimizer Configuration

### 8.1 Optimizer Type & Hyperparameters
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `optim` | `adamw` | ğŸ”µ `adamw` | ğŸ”µ `adamw` | Same optimizer |
| `optim_conf.lr` | `5.0e-06` | ğŸŸ¡ **`5e-5`** | ğŸŸ¡ **`1e-4`** | ğŸ”´ **MODIFIED**: Higher LR for fine-tuning |
| `optim_conf.weight_decay` | `0.0` | ğŸŸ¡ **`0.01`** | ğŸŸ¡ **`0.01`** | ğŸ”´ **MODIFIED**: Added weight decay |
| `optim_conf.betas` | âšª N/A | ğŸŸ¡ `[0.9, 0.999]` | âšª OMITTED | ğŸŸ  **NEW**: Beta parameters specified in config_8k_telephone |

---

## 9. Learning Rate Scheduler

### 9.1 Scheduler Configuration
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `scheduler` | `warmuplr` | ğŸ”µ `warmuplr` | ğŸ”µ `warmuplr` | Same scheduler type |
| `scheduler_conf.warmup_steps` | `2500` | ğŸŸ¡ **`1000`** | ğŸŸ¡ **`1000`** | ğŸŸ¡ **MODIFIED**: Shorter warmup |

---

## 10. Dataset Configuration

### 10.1 Dataset Type & Batch Configuration
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `dataset` | `FunASR` | ğŸ”µ `FunASR` | ğŸ”µ `FunASR` | Same dataset type |
| `dataset_conf.index_ds` | `FunASR` | ğŸ”µ `FunASR` | ğŸ”µ `FunASR` | Index dataset type |
| `dataset_conf.batch_sampler` | `BatchSampler` | ğŸ”µ `BatchSampler` | ğŸ”µ `BatchSampler` | Batch sampling strategy |
| `dataset_conf.batch_type` | `token` | ğŸ”µ `token` | ğŸ”µ `token` | Token-based batching |
| `dataset_conf.batch_size` | `6000` | ğŸ”µ `6000` | ğŸ”µ `6000` | Batch size (tokens) |
| `dataset_conf.max_token_length` | `3500` | ğŸŸ¡ **`1024`** | ğŸ”µ `3500` | ğŸŸ¡ **MODIFIED**: Shorter max length in config_8k_telephone |
| `dataset_conf.shuffle` | `true` | ğŸ”µ `true` | ğŸ”µ `true` | Shuffle data |
| `dataset_conf.sort_size` | `1024` | ğŸ”µ `1024` | ğŸ”µ `1024` | Sort buffer size |
| `dataset_conf.batch_size_scale_ratio_max` | `2` | ğŸ”µ `2` | âšª OMITTED | Uses default |

### 10.2 Data Loading & Processing
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `dataset_conf.num_workers` | `4` | ğŸ”µ `4` | ğŸŸ¡ **`8`** | ğŸŸ¡ **MODIFIED**: More workers in stage1 |
| `dataset_conf.audio_adaptor_downsample_rate` | `${audio_adaptor_conf.downsample_rate}` | ğŸ”µ Same | ğŸ”µ Same | References adaptor config |
| `dataset_conf.audio_encoder_downsample_rate` | `6` | ğŸŸ¡ **`2`** | ğŸ”µ `2` | ğŸŸ¡ **MODIFIED**: Different downsampling rate |
| `dataset_conf.data_split_num` | `256` | ğŸŸ¡ **`512`** | ğŸ”µ `256` | ğŸŸ¡ **MODIFIED**: More splits in config_8k_telephone |
| `dataset_conf.batch_size_sample_max` | `10` | ğŸŸ¡ **`15`** | ğŸ”µ `10` | ğŸŸ¡ **MODIFIED**: Larger sample batch in config_8k_telephone |
| `dataset_conf.retry` | `2000` | ğŸŸ¡ **`20`** | ğŸ”µ `2000` | ğŸŸ¡ **MODIFIED**: Fewer retries in config_8k_telephone |

### 10.3 Sequence Length Configuration
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `dataset_conf.batch_size_token_max` | `6000` | âšª OMITTED | ğŸ”µ `6000` | Max tokens per batch |
| `dataset_conf.max_source_length` | `12000` | âšª OMITTED | ğŸ”µ `12000` | Max source length |
| `dataset_conf.max_target_length` | `2048` | âšª OMITTED | ğŸ”µ `2048` | Max target length |
| `dataset_conf.min_source_length` | `10` | âšª OMITTED | ğŸ”µ `10` | Min source length |
| `dataset_conf.batch_size_scale_threshold` | `3000` | âšª OMITTED | ğŸ”µ `3000` | Batch scaling threshold |

### 10.4 Prompt & Hotword Configuration
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `dataset_conf.prompt_classes` | `MultiContextPrompt` | âšª OMITTED | ğŸ”µ `MultiContextPrompt` | Multi-context prompting |
| `dataset_conf.prompt_conf.max_neg_hotwords_num` | `0` | âšª OMITTED | ğŸ”µ `0` | Max negative hotwords |
| `dataset_conf.prompt_conf.min_neg_hotwords_num` | `0` | âšª OMITTED | ğŸ”µ `0` | Min negative hotwords |
| `dataset_conf.prompt_conf.use_hist` | `false` | âšª OMITTED | ğŸ”µ `false` | Use history |
| `dataset_conf.prompt_conf.use_one_pass_result` | `true` | âšª OMITTED | ğŸ”µ `true` | Use one-pass result |
| `dataset_conf.prompt_conf.use_hotwords` | `true` | âšª OMITTED | ğŸ”µ `true` | Use hotwords |
| `dataset_conf.prompt_conf.use_asr_hotwords` | `true` | âšª OMITTED | ğŸ”µ `true` | Use ASR hotwords |
| `dataset_conf.prompt_conf.chinese_hotwords_list` | `null` | âšª OMITTED | ğŸ”µ `null` | Chinese hotwords |
| `dataset_conf.prompt_conf.english_hotwords_list` | `null` | âšª OMITTED | ğŸ”µ `null` | English hotwords |

### 10.5 CTC Tokenizer Configuration
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `dataset_conf.ctc_tokenizer` | `SenseVoiceTokenizer` | âšª OMITTED | ğŸ”µ `SenseVoiceTokenizer` | CTC tokenizer type |
| `dataset_conf.ctc_target_normalize` | `true` | âšª OMITTED | ğŸ”µ `true` | Normalize CTC targets |
| `dataset_conf.ctc_tokenizer_conf.vocab_path` | `null` | âšª OMITTED | ğŸ”µ `null` | Vocabulary path |
| `dataset_conf.ctc_tokenizer_conf.is_multilingual` | `true` | âšª OMITTED | ğŸ”µ `true` | Multilingual support |
| `dataset_conf.ctc_tokenizer_conf.num_languages` | `8749` | âšª OMITTED | ğŸ”µ `8749` | Number of languages |
| `dataset_conf.use_dynamic_output_ratio` | `0.0` | âšª OMITTED | ğŸ”µ `0.0` | Dynamic output ratio |

---

## 11. Tokenizer Configuration

### 11.1 Tokenizer Type & Setup
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `tokenizer` | `HuggingfaceTokenizer` | ğŸ”µ `HuggingfaceTokenizer` | ğŸ”µ `HuggingfaceTokenizer` | Same tokenizer type |
| `tokenizer_conf.init_param_path` | `${llm_conf.init_param_path}` | ğŸŸ¡ `Qwen3-0.6B` | ğŸŸ¡ `Qwen3-0.6B` | ğŸŸ¡ **MODIFIED**: Explicit reference instead of variable |
| `tokenizer_conf.unk_symbol` | âšª N/A | ğŸŸ  **`<unk>`** | ğŸŸ  **`<unk>`** | ğŸŸ  **NEW**: Unknown token symbol |

---

## 12. Other Configuration

### 12.1 Miscellaneous Settings
| Parameter | Pretrained | config_8k_telephone | stage1_encoder_adapt | Notes |
|-----------|-----------|------------------|---------------------|-------|
| `enable_tf32` | `true` | âšª OMITTED | âšª OMITTED | TF32 acceleration |
| `debug` | `false` | âšª OMITTED | âšª OMITTED | Debug mode |
| `train_data_set_list` | `null` | âšª OMITTED | âšª OMITTED | Training dataset list |
| `valid_data_set_list` | `null` | âšª OMITTED | âšª OMITTED | Validation dataset list |
| `init_param` | `null` | âšª OMITTED | âšª OMITTED | Initial parameters path |
| `output_dir` | `null` | âšª OMITTED | âšª OMITTED | Output directory |

---

## Summary of Key Changes for Fine-tuning

### ğŸ”´ Critical Modifications
1. **Audio Encoder Freeze** (`audio_encoder_conf.freeze`): `true` â†’ `false`
   - Encoder becomes trainable during fine-tuning
   
2. **Frontend Sample Rate** (`frontend_conf.fs`): 
   - Pretrained: `16000`
   - config_8k_telephone: `16000`
   - stage1_encoder_adapt: `8000` â† Different by stage

3. **CTC Decoder Freeze** (`ctc_decoder_conf.freeze`):
   - config_8k_telephone: `false` (trainable)
   - stage1_encoder_adapt: `true` (frozen)

4. **Learning Rate** (`optim_conf.lr`):
   - Pretrained: `5e-6` (very small for inference)
   - config_8k_telephone: `5e-5` (10Ã— higher)
   - stage1_encoder_adapt: `1e-4` (20Ã— higher)

5. **Training Epochs** (`train_conf.max_epoch`):
   - Pretrained: `2`
   - Fine-tuning: `10-20` epochs

6. **DeepSpeed** (`train_conf.use_deepspeed`):
   - Pretrained: `true`
   - Fine-tuning: `false` (disabled for smaller experiments)

### ğŸŸ¡ Dataset/Training Adjustments
- **Batch size & sequence length**: Adjusted for memory constraints
- **SpecAugmentation**: Added for data augmentation (not in pretrained)
- **Warmup steps**: Reduced from `2500` to `1000`
- **Weight decay**: Enabled for regularization
- **Data loading workers**: Adjusted by stage (4-8)

### âšª Omitted Parameters (Use Code Defaults)
Many parameters not specified in fine-tuning configs will use defaults from:
- Base configuration file
- Model class defaults
- Code implementation

This includes encoder-specific parameters (dropout rates, attention heads, etc.) that remain unchanged from pretrained model.

---

## How to Use This Document

1. **Verify alignment before training**: Compare your config with the pretrained model
2. **Understand modifications**: Each ğŸŸ¡ marked change has been intentionally modified for fine-tuning
3. **Check omitted parameters**: If a parameter is not in your config (âšª), verify code defaults are acceptable
4. **Stage-specific differences**: Note differences between stage1_encoder_adapt and config_8k_telephone
5. **Document your customizations**: If you modify any parameters, update this document

